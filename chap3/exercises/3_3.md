> Consider the following CUDA kernel and the corresponding host function that calls it:

```c++
01  __global__ void foo_kernel(float* a, float* b, unsigned int M, unsigned int N) {
02      unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;
03      unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;
04      if (row < M && col < N) {
05          b[row*N + col] = a[row*N + col]/2.1f + 4.8f;
06      }
07  }
08  void foo(float* a_d, float* b_d) {
09      unsigned int M = 150;
10      unsigned int N = 300;
11      dim3 bd(16, 32);
12      dim3 gd((N - 1) / 16 + 1, (M - 1) / 32 + 1);
13      foo_kernel <<<gd, bd>>> (a_d, b_d, M, N);
14  }
```

> a. What is the number of threads per block?

100

> b. What is the number of threads in the grid?

51200

> c. What is the number of blocks in the grid?

512

> d. What is the number of threads that execute the code on line 05?

45000
